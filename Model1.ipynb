{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tim Kaine Shows Trump DECENT Way To Respond A...</td>\n",
       "      <td>Senator Tim Kaine (D   VA) , Hillary Clinton s...</td>\n",
       "      <td>News</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMERICAN NANNY Helping Muslim Refugees Found B...</td>\n",
       "      <td>Is the government of Austria harboring violent...</td>\n",
       "      <td>Government News</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump presidency faces longer odds after Iowa,...</td>\n",
       "      <td>NEW YORK   - Republican candidate Donald Trump...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Red Cross says life has stopped in Myanmar's R...</td>\n",
       "      <td>GENEVA   - Life has stopped in its tracks in M...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Obama And Justin Trudeau Had Dinner Tuesday; ...</td>\n",
       "      <td>As the nation is embroiled in one scandal afte...</td>\n",
       "      <td>News</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Tim Kaine Shows Trump DECENT Way To Respond A...   \n",
       "1  AMERICAN NANNY Helping Muslim Refugees Found B...   \n",
       "2  Trump presidency faces longer odds after Iowa,...   \n",
       "3  Red Cross says life has stopped in Myanmar's R...   \n",
       "4   Obama And Justin Trudeau Had Dinner Tuesday; ...   \n",
       "\n",
       "                                                text          subject  News  \n",
       "0  Senator Tim Kaine (D   VA) , Hillary Clinton s...             News  fake  \n",
       "1  Is the government of Austria harboring violent...  Government News  fake  \n",
       "2  NEW YORK   - Republican candidate Donald Trump...     politicsNews  real  \n",
       "3  GENEVA   - Life has stopped in its tracks in M...        worldnews  real  \n",
       "4  As the nation is embroiled in one scandal afte...             News  fake  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"FakeReal_NewsData.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44689, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "News\n",
       "fake    23478\n",
       "real    21211\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.News.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title      0\n",
       "text       0\n",
       "subject    0\n",
       "News       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>News</th>\n",
       "      <th>merged_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tim Kaine Shows Trump DECENT Way To Respond A...</td>\n",
       "      <td>Senator Tim Kaine (D   VA) , Hillary Clinton s...</td>\n",
       "      <td>News</td>\n",
       "      <td>fake</td>\n",
       "      <td>Tim Kaine Shows Trump DECENT Way To Respond A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMERICAN NANNY Helping Muslim Refugees Found B...</td>\n",
       "      <td>Is the government of Austria harboring violent...</td>\n",
       "      <td>Government News</td>\n",
       "      <td>fake</td>\n",
       "      <td>AMERICAN NANNY Helping Muslim Refugees Found B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump presidency faces longer odds after Iowa,...</td>\n",
       "      <td>NEW YORK   - Republican candidate Donald Trump...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>real</td>\n",
       "      <td>Trump presidency faces longer odds after Iowa,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Red Cross says life has stopped in Myanmar's R...</td>\n",
       "      <td>GENEVA   - Life has stopped in its tracks in M...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>real</td>\n",
       "      <td>Red Cross says life has stopped in Myanmar's R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Obama And Justin Trudeau Had Dinner Tuesday; ...</td>\n",
       "      <td>As the nation is embroiled in one scandal afte...</td>\n",
       "      <td>News</td>\n",
       "      <td>fake</td>\n",
       "      <td>Obama And Justin Trudeau Had Dinner Tuesday; ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Tim Kaine Shows Trump DECENT Way To Respond A...   \n",
       "1  AMERICAN NANNY Helping Muslim Refugees Found B...   \n",
       "2  Trump presidency faces longer odds after Iowa,...   \n",
       "3  Red Cross says life has stopped in Myanmar's R...   \n",
       "4   Obama And Justin Trudeau Had Dinner Tuesday; ...   \n",
       "\n",
       "                                                text          subject  News  \\\n",
       "0  Senator Tim Kaine (D   VA) , Hillary Clinton s...             News  fake   \n",
       "1  Is the government of Austria harboring violent...  Government News  fake   \n",
       "2  NEW YORK   - Republican candidate Donald Trump...     politicsNews  real   \n",
       "3  GENEVA   - Life has stopped in its tracks in M...        worldnews  real   \n",
       "4  As the nation is embroiled in one scandal afte...             News  fake   \n",
       "\n",
       "                                          merged_txt  \n",
       "0   Tim Kaine Shows Trump DECENT Way To Respond A...  \n",
       "1  AMERICAN NANNY Helping Muslim Refugees Found B...  \n",
       "2  Trump presidency faces longer odds after Iowa,...  \n",
       "3  Red Cross says life has stopped in Myanmar's R...  \n",
       "4   Obama And Justin Trudeau Had Dinner Tuesday; ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"merged_txt\"] = df['title'] + \" \" + df['text']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>News</th>\n",
       "      <th>merged_txt</th>\n",
       "      <th>real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tim Kaine Shows Trump DECENT Way To Respond A...</td>\n",
       "      <td>Senator Tim Kaine (D   VA) , Hillary Clinton s...</td>\n",
       "      <td>News</td>\n",
       "      <td>fake</td>\n",
       "      <td>Tim Kaine Shows Trump DECENT Way To Respond A...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMERICAN NANNY Helping Muslim Refugees Found B...</td>\n",
       "      <td>Is the government of Austria harboring violent...</td>\n",
       "      <td>Government News</td>\n",
       "      <td>fake</td>\n",
       "      <td>AMERICAN NANNY Helping Muslim Refugees Found B...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump presidency faces longer odds after Iowa,...</td>\n",
       "      <td>NEW YORK   - Republican candidate Donald Trump...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>real</td>\n",
       "      <td>Trump presidency faces longer odds after Iowa,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Red Cross says life has stopped in Myanmar's R...</td>\n",
       "      <td>GENEVA   - Life has stopped in its tracks in M...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>real</td>\n",
       "      <td>Red Cross says life has stopped in Myanmar's R...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Obama And Justin Trudeau Had Dinner Tuesday; ...</td>\n",
       "      <td>As the nation is embroiled in one scandal afte...</td>\n",
       "      <td>News</td>\n",
       "      <td>fake</td>\n",
       "      <td>Obama And Justin Trudeau Had Dinner Tuesday; ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Tim Kaine Shows Trump DECENT Way To Respond A...   \n",
       "1  AMERICAN NANNY Helping Muslim Refugees Found B...   \n",
       "2  Trump presidency faces longer odds after Iowa,...   \n",
       "3  Red Cross says life has stopped in Myanmar's R...   \n",
       "4   Obama And Justin Trudeau Had Dinner Tuesday; ...   \n",
       "\n",
       "                                                text          subject  News  \\\n",
       "0  Senator Tim Kaine (D   VA) , Hillary Clinton s...             News  fake   \n",
       "1  Is the government of Austria harboring violent...  Government News  fake   \n",
       "2  NEW YORK   - Republican candidate Donald Trump...     politicsNews  real   \n",
       "3  GENEVA   - Life has stopped in its tracks in M...        worldnews  real   \n",
       "4  As the nation is embroiled in one scandal afte...             News  fake   \n",
       "\n",
       "                                          merged_txt  real  \n",
       "0   Tim Kaine Shows Trump DECENT Way To Respond A...     0  \n",
       "1  AMERICAN NANNY Helping Muslim Refugees Found B...     0  \n",
       "2  Trump presidency faces longer odds after Iowa,...     1  \n",
       "3  Red Cross says life has stopped in Myanmar's R...     1  \n",
       "4   Obama And Justin Trudeau Had Dinner Tuesday; ...     0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['real'] = df['News'].apply(lambda x: 1 if x=='real' else 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^\\w\\s\\’]' , \" \", text)\n",
    "    text = re.sub(r'[ \\n]+' , \" \", text)\n",
    "    text = text.strip().lower()\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        \n",
    "        filtered_tokens.append(token.lemma_)\n",
    "    \n",
    "    return \" \".join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_txt'] = df['merged_txt'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35748,)  ...  (8937,)  ...  (35748,)  ...  (8937,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.processed_txt, df.real, test_size=0.2, random_state = 42)\n",
    "print(X_train.shape,\" ... \", X_test.shape,\" ... \", y_train.shape,\" ... \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scores = {}\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting XGBoost model...\n",
      "fitting Multinomial Naive Bayes model...\n",
      "fitting Random Forest model...\n",
      "fitting Decision Tree model...\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'XGBoost' : XGBClassifier(random_state=42),\n",
    "    'Multinomial Naive Bayes': MultinomialNB(),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    \n",
    "    # Train the model\n",
    "    print(f'fitting {model_name} model...')\n",
    "    model.fit(X_train_vec, y_train)\n",
    "    \n",
    "    # Predict and calculate accuracy\n",
    "    y_pred = model.predict(X_test_vec)\n",
    "    accuracy_scores[model_name] = accuracy_score(y_test, y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Scores:\n",
      "XGBoost: 0.9932\n",
      "Multinomial Naive Bayes: 0.9515\n",
      "Random Forest: 0.9794\n",
      "Decision Tree: 0.9593\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Scores:\")\n",
    "for model_name, score in accuracy_scores.items():\n",
    "    print(f\"{model_name}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost model...\n",
      "XGBoost Fold Accuracies: [0.9889510489510489, 0.99006993006993, 0.9934265734265735, 0.9874108266890474, 0.9906280598685131]\n",
      "XGBoost Mean Cross-Validation Accuracy: 0.9901\n",
      "Training Random Forest model...\n",
      "Random Forest Fold Accuracies: [0.9791608391608392, 0.9783216783216783, 0.9822377622377623, 0.97580081130228, 0.9801370821093859]\n",
      "Random Forest Mean Cross-Validation Accuracy: 0.9791\n",
      "\n",
      "Overall Cross-Validation Accuracy Scores:\n",
      "XGBoost: 0.9901\n",
      "Random Forest: 0.9791\n"
     ]
    }
   ],
   "source": [
    "cv_scores ={}\n",
    "models = {\n",
    "    'XGBoost' : XGBClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# 5-Fold Cross Validation using KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name} model...\")\n",
    "    \n",
    "    fold_accuracies = []  # To store accuracy for each fold\n",
    "    \n",
    "    # Split the data into train and validation sets for each fold\n",
    "    for train_index, val_index in kf.split(X_train_vec):\n",
    "        # Split into training and validation sets\n",
    "        X_train_fold, X_val_fold = X_train_vec[train_index], X_train_vec[val_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Predict on the validation fold\n",
    "        y_pred = model.predict(X_val_fold)\n",
    "        \n",
    "        # Calculate accuracy for the current fold\n",
    "        accuracy = accuracy_score(y_val_fold, y_pred)\n",
    "        fold_accuracies.append(accuracy)\n",
    "    \n",
    "    # Store the mean accuracy for the model\n",
    "    mean_accuracy = np.mean(fold_accuracies)\n",
    "    cv_scores[model_name] = mean_accuracy\n",
    "\n",
    "    # Print the fold accuracies and mean accuracy\n",
    "    print(f\"{model_name} Fold Accuracies: {fold_accuracies}\")\n",
    "    print(f\"{model_name} Mean Cross-Validation Accuracy: {mean_accuracy:.4f}\")\n",
    "    \n",
    "# Print the mean Cross-Validation Accuracy Scores for all models\n",
    "print(\"\\nOverall Cross-Validation Accuracy Scores:\")\n",
    "for model_name, score in cv_scores.items():\n",
    "    print(f\"{model_name}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_13\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_13\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">9,939,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_13 (\u001b[38;5;33mEmbedding\u001b[0m)        │ ?                      │     \u001b[38;5;34m9,939,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_26 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_26 (\u001b[38;5;33mLSTM\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_27 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,939,100</span> (37.91 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,939,100\u001b[0m (37.91 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,939,100</span> (37.91 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m9,939,100\u001b[0m (37.91 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/15\n",
      "1118/1118 - 64s - 58ms/step - accuracy: 0.8626 - loss: 0.3241 - val_accuracy: 0.9194 - val_loss: 0.2193\n",
      "Epoch 2/15\n",
      "1118/1118 - 66s - 59ms/step - accuracy: 0.9126 - loss: 0.2235 - val_accuracy: 0.9063 - val_loss: 0.2399\n",
      "Epoch 3/15\n",
      "1118/1118 - 63s - 56ms/step - accuracy: 0.9426 - loss: 0.1546 - val_accuracy: 0.9631 - val_loss: 0.1020\n",
      "Epoch 4/15\n",
      "1118/1118 - 58s - 52ms/step - accuracy: 0.9578 - loss: 0.1147 - val_accuracy: 0.9670 - val_loss: 0.0932\n",
      "Epoch 5/15\n",
      "1118/1118 - 60s - 53ms/step - accuracy: 0.9679 - loss: 0.0886 - val_accuracy: 0.9676 - val_loss: 0.0809\n",
      "Epoch 6/15\n",
      "1118/1118 - 67s - 60ms/step - accuracy: 0.9746 - loss: 0.0696 - val_accuracy: 0.9800 - val_loss: 0.0543\n",
      "Epoch 7/15\n",
      "1118/1118 - 61s - 55ms/step - accuracy: 0.9794 - loss: 0.0565 - val_accuracy: 0.9844 - val_loss: 0.0465\n",
      "Epoch 8/15\n",
      "1118/1118 - 65s - 58ms/step - accuracy: 0.9825 - loss: 0.0502 - val_accuracy: 0.9861 - val_loss: 0.0448\n",
      "Epoch 9/15\n",
      "1118/1118 - 67s - 60ms/step - accuracy: 0.9844 - loss: 0.0426 - val_accuracy: 0.9787 - val_loss: 0.0614\n",
      "Epoch 10/15\n",
      "1118/1118 - 75s - 67ms/step - accuracy: 0.9866 - loss: 0.0385 - val_accuracy: 0.9885 - val_loss: 0.0363\n",
      "Epoch 11/15\n",
      "1118/1118 - 69s - 62ms/step - accuracy: 0.9887 - loss: 0.0312 - val_accuracy: 0.9879 - val_loss: 0.0399\n",
      "Epoch 12/15\n",
      "1118/1118 - 64s - 57ms/step - accuracy: 0.9904 - loss: 0.0277 - val_accuracy: 0.9879 - val_loss: 0.0346\n",
      "Epoch 13/15\n",
      "1118/1118 - 58s - 52ms/step - accuracy: 0.9914 - loss: 0.0252 - val_accuracy: 0.9881 - val_loss: 0.0386\n",
      "Epoch 14/15\n",
      "1118/1118 - 59s - 52ms/step - accuracy: 0.9924 - loss: 0.0218 - val_accuracy: 0.9878 - val_loss: 0.0398\n",
      "Epoch 15/15\n",
      "1118/1118 - 62s - 55ms/step - accuracy: 0.9928 - loss: 0.0202 - val_accuracy: 0.9904 - val_loss: 0.0401\n",
      "\u001b[1m280/280\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 22ms/step\n",
      "LSTM Model Accuracy: 0.9904\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Parameters\n",
    "embedding_dim = 100  # Size of the embedding vectors\n",
    "max_len = 100  # Maximum sequence length\n",
    "vocab_size = 20000  # Example vocabulary size (can be changed)\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already available\n",
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)  # Fit tokenizer on training data\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# Load GloVe embeddings\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "with open(r'E:\\Repositories_Data-Science\\Downloads\\glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefficients = np.asarray(values[1:], dtype='float32')\n",
    "        if word in tokenizer.word_index:\n",
    "            idx = tokenizer.word_index[word]\n",
    "            embedding_matrix[idx] = coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the LSTM model using Sequential\n",
    "lstm = Sequential([\n",
    "    Embedding(\n",
    "        input_dim=len(tokenizer.word_index) + 1,  # Vocabulary size\n",
    "        output_dim=embedding_dim,  # Embedding vector size\n",
    "        weights=[embedding_matrix],  # Pre-trained GloVe embeddings\n",
    "        trainable=False  # Keep embeddings fixed\n",
    "        input_length = max_len\n",
    "    ),\n",
    "    Dropout(0.3),\n",
    "    LSTM(128),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "lstm.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "lstm.build()\n",
    "print(lstm.summary())\n",
    "# Train the model\n",
    "history = lstm.fit(\n",
    "    X_train_pad, y_train,\n",
    "    validation_data=(X_test_pad, y_test),\n",
    "    epochs=15,  # Adjust epochs as needed\n",
    "    batch_size=32,  # Adjust batch size as needed\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = (lstm.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"LSTM Model Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      4703\n",
      "           1       0.99      0.98      0.99      4234\n",
      "\n",
      "    accuracy                           0.99      8937\n",
      "   macro avg       0.99      0.99      0.99      8937\n",
      "weighted avg       0.99      0.99      0.99      8937\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      4703\n",
      "           1       0.99      0.99      0.99      4234\n",
      "\n",
      "    accuracy                           0.99      8937\n",
      "   macro avg       0.99      0.99      0.99      8937\n",
      "weighted avg       0.99      0.99      0.99      8937\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = models['XGBoost'].predict(X_test_vec)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
